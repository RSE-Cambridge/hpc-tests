                      :-) GROMACS - gmx mdrun, 2016.4 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra    Gerrit Groenhof  
 Christoph Junghans   Anca Hamuraru    Vincent Hindriksen Dimitrios Karkoulis
    Peter Kasson        Jiri Kraus      Carsten Kutzner      Per Larsson    
  Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff    Erik Marklund   
   Teemu Murtola       Szilard Pall       Sander Pronk      Roland Schulz   
  Alexey Shvetsov     Michael Shirts     Alfons Sijbers     Peter Tieleman  
  Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2016.4
Executable:   /alaska/steveb/spack/opt/spack/linux-centos7-broadwell/gcc-9.3.0/gromacs-2016.4-y5sjbs44dv3c5ky2godzmvtt7doh4e7f/bin/gmx_mpi
Data prefix:  /alaska/steveb/spack/opt/spack/linux-centos7-broadwell/gcc-9.3.0/gromacs-2016.4-y5sjbs44dv3c5ky2godzmvtt7doh4e7f
Working dir:  /alaska/steveb/hpc-tests/stage/alaska/ib-gcc9-openmpi4-ucx/gromacs/Gromacs_61k_4
Command line:
  gmx_mpi mdrun -s benchmark.tpr -g 61k-atoms.log -noconfout


Running on 4 nodes with total 128 cores, 256 logical cores
  Cores per node:           32
  Logical cores per node:   64
Hardware detected on host openhpc-compute-2 (the node of MPI rank 0):
  CPU info:
    Vendor: Intel
    Brand:  Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

  Hardware topology: Full, with devices

Reading file benchmark.tpr, VERSION 5.1.4 (single precision)
Note: file tpx version 103, software tpx version 110
Changing nstlist from 10 to 20, rlist from 1.2 to 1.224


Will use 96 particle-particle and 32 PME only ranks
This is a guess, check the performance at the end of the log file
Using 128 MPI processes
Using 2 OpenMP threads per MPI process

starting mdrun 'Protein'
50000 steps,    100.0 ps.

step 6000 Turning on dynamic load balancing, because the performance loss due to load imbalance is 2.1 %.


step 6400 Turning off dynamic load balancing, because it is degrading performance.


step 12000 Turning on dynamic load balancing, because the performance loss due to load imbalance is 5.5 %.


step 12400 Turning off dynamic load balancing, because it is degrading performance.


step 14000 Will no longer try dynamic load balancing, as it degraded performance.


 Average load imbalance: 26.1 %
 Part of the total run time spent waiting due to load imbalance: 6.4 %
 Average PME mesh/force load: 1.021
 Part of the total run time spent waiting due to PP/PME imbalance: 0.8 %

NOTE: 6.4 % of the available CPU time was lost due to load imbalance
      in the domain decomposition.
      You might want to use dynamic load balancing (option -dlb.)


               Core t (s)   Wall t (s)        (%)
       Time:    56196.479      219.517    25600.0
                 (ns/day)    (hour/ns)
Performance:       39.360        0.610

GROMACS reminds you: "Is That a Real Poncho ?" (F. Zappa)


real	3m47.260s
user	0m0.008s
sys	0m0.029s
